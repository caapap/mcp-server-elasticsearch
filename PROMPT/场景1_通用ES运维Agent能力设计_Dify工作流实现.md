# åœºæ™¯ 1: é€šç”¨ ES æ•…éšœæ’æŸ¥ä¸æ•°æ®æŸ¥è¯¢ - Dify å·¥ä½œæµå®ç°æ–¹æ¡ˆ

> **è®¾è®¡ç›®æ ‡**: å°† Agent æ¨ç†é“¾è½¬åŒ–ä¸ºå¯æ‰§è¡Œã€å¯ç›‘æ§ã€å¯å¤ç”¨çš„ Dify Workflow

---

## ç›®å½•
1. [åŸºç¡€èƒ½åŠ›çŸ©é˜µ](#ä¸€åŸºç¡€èƒ½åŠ›çŸ©é˜µ)
2. [Dify Workflow æ¶æ„è®¾è®¡](#äºŒdify-workflow-æ¶æ„è®¾è®¡)
3. [èƒ½åŠ› 1: è‡ªåŠ¨åŒ–æ•…éšœè¯Šæ–­å·¥ä½œæµ](#ä¸‰èƒ½åŠ›-1-è‡ªåŠ¨åŒ–æ•…éšœè¯Šæ–­ä¸æ ¹å› å®šä½)
4. [èƒ½åŠ› 2: æ™ºèƒ½æ•°æ®æ¢ç´¢å·¥ä½œæµ](#å››èƒ½åŠ›-2-æ™ºèƒ½æ•°æ®æ¢ç´¢ä¸æ´å¯Ÿæå–)
5. [èƒ½åŠ› 3: ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢å·¥ä½œæµ](#äº”èƒ½åŠ›-3-ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯å¼æŸ¥è¯¢)
6. [èƒ½åŠ› 4: å¤æ‚å…³è”åˆ†æå·¥ä½œæµ](#å…­èƒ½åŠ›-4-å¤æ‚å…³è”åˆ†æ-cross-index)
7. [é€šç”¨ç»„ä»¶ä¸æœ€ä½³å®è·µ](#ä¸ƒé€šç”¨ç»„ä»¶ä¸æœ€ä½³å®è·µ)
8. [éƒ¨ç½²ä¸ç›‘æ§](#å…«éƒ¨ç½²ä¸ç›‘æ§)

---

## ä¸€ã€åŸºç¡€èƒ½åŠ›çŸ©é˜µ

| èƒ½åŠ›ç»´åº¦ | æ¶‰åŠå·¥å…· | Dify å®ç°éš¾åº¦ | é¢„ä¼°èŠ‚ç‚¹æ•° |
|---------|---------|--------------|-----------|
| **é›†ç¾¤å¥åº·è¯Šæ–­** | `get_cluster_health`, `get_nodes_info`, `get_shards` | â­â­â­ | 8-12 |
| **ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†** | `list_indices`, `list_indices_detailed`, `get_mappings` | â­â­ | 6-8 |
| **æ•°æ®æ£€ç´¢ä¸åˆ†æ** | `esql`, `search` | â­â­â­â­ | 10-15 |
| **æ ¹å› åˆ†æ** | å¤šå·¥å…·ç»„åˆæ¨ç† | â­â­â­â­â­ | 15-20 |

---

## äºŒã€Dify Workflow æ¶æ„è®¾è®¡

### 2.1 èŠ‚ç‚¹ç±»å‹æ˜ å°„

| Agent æ“ä½œ | Dify èŠ‚ç‚¹ç±»å‹ | è¯´æ˜ |
|-----------|-------------|------|
| è°ƒç”¨ MCP å·¥å…· | **å·¥å…· (Tool)** | ç›´æ¥è°ƒç”¨ `elasticsearch-mcp` çš„å·¥å…· |
| æ¡ä»¶åˆ¤æ–­ | **æ¡ä»¶åˆ†æ”¯ (IF/ELSE)** | æ ¹æ®å·¥å…·è¿”å›å€¼åˆ¤æ–­ä¸‹ä¸€æ­¥ |
| æ•°æ®è§£æ | **ä»£ç æ‰§è¡Œ (Code)** | ä½¿ç”¨ Python/Jinja2 æå–å­—æ®µ |
| æ¨ç†åˆ†æ | **LLM** | è®©æ¨¡å‹åˆ†ææ•°æ®å¹¶ç»™å‡ºç»“è®º |
| ç»“æœèšåˆ | **å˜é‡èšåˆå™¨ (Variable Aggregator)** | åˆå¹¶å¤šä¸ªå·¥å…·çš„è¾“å‡º |
| å¾ªç¯å¤„ç† | **è¿­ä»£å™¨ (Iterator)** | éå†ç´¢å¼•åˆ—è¡¨æˆ–åˆ†ç‰‡åˆ—è¡¨ |

### 2.2 å˜é‡å‘½åè§„èŒƒ

ä¸ºäº†åœ¨å¤æ‚å·¥ä½œæµä¸­ä¿æŒæ¸…æ™°ï¼Œå»ºè®®é‡‡ç”¨ä»¥ä¸‹å‘½åè§„èŒƒï¼š

```
å·¥ä½œæµå˜é‡:
  - workflow.user_query       (ç”¨æˆ·è¾“å…¥)
  - workflow.cluster_status   (é›†ç¾¤çŠ¶æ€: green/yellow/red)
  - workflow.target_index     (ç›®æ ‡ç´¢å¼•å)
  - workflow.error_occurred   (æ˜¯å¦å‘ç”Ÿé”™è¯¯)

èŠ‚ç‚¹è¾“å‡ºå˜é‡:
  - health_check.status       (å¥åº·æ£€æŸ¥èŠ‚ç‚¹çš„è¾“å‡º)
  - shard_info.unassigned     (æœªåˆ†é…åˆ†ç‰‡åˆ—è¡¨)
  - llm_analysis.root_cause   (LLM åˆ†æçš„æ ¹å› )
```

### 2.3 é”™è¯¯å¤„ç†ç­–ç•¥

åœ¨æ¯ä¸ªå…³é”®å·¥å…·èŠ‚ç‚¹åæ·»åŠ  **IF/ELSE åˆ†æ”¯**ï¼š
```
å·¥å…·è°ƒç”¨æˆåŠŸ (HTTP 200, æœ‰æ•°æ®) â†’ ç»§ç»­æ‰§è¡Œ
å·¥å…·è°ƒç”¨å¤±è´¥ (è¶…æ—¶/é”™è¯¯)      â†’ è·³è½¬åˆ°ã€é”™è¯¯å¤„ç†åˆ†æ”¯ã€‘
                              â”œâ”€ è®°å½•é”™è¯¯ä¿¡æ¯
                              â”œâ”€ LLM ç”Ÿæˆæ’æŸ¥å»ºè®®
                              â””â”€ æå‰ç»“æŸå·¥ä½œæµ
```

---

## ä¸‰ã€èƒ½åŠ› 1: è‡ªåŠ¨åŒ–æ•…éšœè¯Šæ–­ä¸æ ¹å› å®šä½

### 3.1 å…¸å‹åœºæ™¯
**ç”¨æˆ·è¾“å…¥**: "é›†ç¾¤å˜æ…¢äº†ï¼Œå¸®æˆ‘æŸ¥ä¸€ä¸‹æ˜¯ä»€ä¹ˆé—®é¢˜"

### 3.2 Dify Workflow èŠ‚ç‚¹å›¾

```mermaid
graph TD
    Start[å¼€å§‹] --> Input[æ¥æ”¶ç”¨æˆ·è¾“å…¥]
    Input --> Tool1[å·¥å…·: get_cluster_health]
    
    Tool1 --> Check1{æ£€æŸ¥: è°ƒç”¨æˆåŠŸ?}
    Check1 -->|å¤±è´¥| Error1[LLM: ç”Ÿæˆè¿æ¥å¤±è´¥æŠ¥å‘Š]
    Check1 -->|æˆåŠŸ| Parse1[ä»£ç : è§£æ status å­—æ®µ]
    
    Parse1 --> Branch1{æ¡ä»¶åˆ†æ”¯: status å€¼}
    Branch1 -->|red| RedBranch[çº¢è‰²åˆ†æ”¯]
    Branch1 -->|yellow| YellowBranch[é»„è‰²åˆ†æ”¯]
    Branch1 -->|green| GreenBranch[ç»¿è‰²åˆ†æ”¯]
    
    RedBranch --> Tool2[å·¥å…·: list_indices_detailed<br/>health=red]
    Tool2 --> Tool3[å·¥å…·: get_shards<br/>index=æœ€å¤§é—®é¢˜ç´¢å¼•]
    Tool3 --> Tool4[å·¥å…·: get_nodes_info]
    Tool4 --> Aggregate1[å˜é‡èšåˆå™¨:<br/>åˆå¹¶æ‰€æœ‰è¯Šæ–­æ•°æ®]
    
    YellowBranch --> Tool5[å·¥å…·: list_indices_detailed<br/>health=yellow]
    Tool5 --> Tool6[å·¥å…·: get_shards]
    Tool6 --> Aggregate1
    
    GreenBranch --> Tool7[å·¥å…·: get_nodes_info]
    Tool7 --> Code1[ä»£ç : åˆ†æèŠ‚ç‚¹æ€§èƒ½æŒ‡æ ‡]
    Code1 --> Aggregate1
    
    Aggregate1 --> LLM1[LLM: æ ¹å› åˆ†æä¸æŠ¥å‘Šç”Ÿæˆ]
    LLM1 --> End[ç»“æŸ: è¾“å‡ºè¯Šæ–­æŠ¥å‘Š]
    
    Error1 --> End
```

### 3.3 å…³é”®èŠ‚ç‚¹é…ç½®

#### èŠ‚ç‚¹ 1: å·¥å…· - get_cluster_health
```yaml
èŠ‚ç‚¹åç§°: health_check
èŠ‚ç‚¹ç±»å‹: å·¥å…· (Tool)
å·¥å…·: elasticsearch-mcp.get_cluster_health
å‚æ•°: {} (æ— å‚æ•°)
è¶…æ—¶æ—¶é—´: 10 ç§’
è¾“å‡ºå˜é‡: 
  - health_check.result (å®Œæ•´ JSON)
  - health_check.status (æå–: result.status)
é”™è¯¯å¤„ç†: å¯ç”¨ (å¤±è´¥æ—¶ç»§ç»­)
```

#### èŠ‚ç‚¹ 2: æ¡ä»¶åˆ†æ”¯ - status åˆ¤æ–­
```yaml
èŠ‚ç‚¹åç§°: status_branch
èŠ‚ç‚¹ç±»å‹: æ¡ä»¶åˆ†æ”¯ (IF/ELSE)
æ¡ä»¶é…ç½®:
  - IF: {{health_check.status}} == "red"
    THEN: è·³è½¬åˆ° red_path
  - ELIF: {{health_check.status}} == "yellow"
    THEN: è·³è½¬åˆ° yellow_path
  - ELSE: è·³è½¬åˆ° green_path
```

#### èŠ‚ç‚¹ 3: ä»£ç æ‰§è¡Œ - æå–æœ€å¤§é—®é¢˜ç´¢å¼•
```yaml
èŠ‚ç‚¹åç§°: extract_largest_index
èŠ‚ç‚¹ç±»å‹: ä»£ç æ‰§è¡Œ (Code)
è¾“å…¥å˜é‡: {{indices_detailed.result}}
ä»£ç è¯­è¨€: Python
ä»£ç å†…å®¹:
```python
import json

# è¾“å…¥: indices_detailed.result (JSON æ•°ç»„)
indices = json.loads(indices_detailed_result)

# æŒ‰ docs.count æ’åº
sorted_indices = sorted(
    indices, 
    key=lambda x: int(x.get("docs.count", 0)), 
    reverse=True
)

# è¿”å›æœ€å¤§çš„ç´¢å¼•å
return {
    "largest_index": sorted_indices[0]["index"] if sorted_indices else None,
    "count": len(sorted_indices)
}
```
è¾“å‡ºå˜é‡:
  - extract.largest_index
  - extract.count
```

#### èŠ‚ç‚¹ 4: LLM - æ ¹å› åˆ†æ
```yaml
èŠ‚ç‚¹åç§°: root_cause_analysis
èŠ‚ç‚¹ç±»å‹: LLM
æ¨¡å‹: gpt-4o æˆ– claude-3.5-sonnet
System Prompt:
  ä½ æ˜¯ ES è¿ç»´ä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹è¯Šæ–­æ•°æ®ï¼Œåˆ†ææ ¹æœ¬åŸå› ï¼š
  
  1. é›†ç¾¤çŠ¶æ€: {{health_check.status}}
  2. é—®é¢˜ç´¢å¼•: {{extract.largest_index}}
  3. åˆ†ç‰‡ä¿¡æ¯: {{shard_info.result}}
  4. èŠ‚ç‚¹ä¿¡æ¯: {{nodes_info.result}}
  
  æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡º:
  ## æ ¹å› å®šä½
  [ä¸€å¥è¯æ€»ç»“]
  
  ## å½±å“èŒƒå›´
  - [å…·ä½“å½±å“]
  
  ## ç´§æ€¥æªæ–½
  1. [æ­¥éª¤ 1]
  2. [æ­¥éª¤ 2]
  
  ## é¢„é˜²æªæ–½
  - [å»ºè®®]

User Message: è¯·åˆ†æ
è¾“å‡ºå˜é‡: llm_analysis.text
```

### 3.4 å®Œæ•´å·¥ä½œæµå˜é‡è¡¨

| å˜é‡å | ç±»å‹ | æ¥æºèŠ‚ç‚¹ | ç”¨é€” |
|-------|------|---------|------|
| `workflow.user_query` | String | å¼€å§‹èŠ‚ç‚¹ | ç”¨æˆ·åŸå§‹è¾“å…¥ |
| `health_check.status` | String | get_cluster_health | é›†ç¾¤çŠ¶æ€ (red/yellow/green) |
| `health_check.nodes` | Integer | get_cluster_health | èŠ‚ç‚¹æ•°é‡ |
| `indices_detailed.result` | Array | list_indices_detailed | ç´¢å¼•åˆ—è¡¨ (JSON) |
| `extract.largest_index` | String | ä»£ç æ‰§è¡Œ | æœ€å¤§é—®é¢˜ç´¢å¼•å |
| `shard_info.result` | Array | get_shards | åˆ†ç‰‡è¯¦æƒ… |
| `nodes_info.result` | Object | get_nodes_info | èŠ‚ç‚¹è¯¦æƒ… |
| `llm_analysis.text` | String | LLM èŠ‚ç‚¹ | æœ€ç»ˆè¯Šæ–­æŠ¥å‘Š |

---

## å››ã€èƒ½åŠ› 2: æ™ºèƒ½æ•°æ®æ¢ç´¢ä¸æ´å¯Ÿæå–

### 4.1 å…¸å‹åœºæ™¯
**ç”¨æˆ·è¾“å…¥**: "å¸®æˆ‘åˆ†æä¸€ä¸‹æœ€è¿‘ä¸€å‘¨çš„é”™è¯¯æ—¥å¿—è¶‹åŠ¿"

### 4.2 Dify Workflow èŠ‚ç‚¹å›¾

```mermaid
graph TD
    Start[å¼€å§‹] --> LLM1[LLM: æå–ç”¨æˆ·æ„å›¾]
    LLM1 --> Extract1[ä»£ç : è§£æå…³é”®è¯<br/>æ—¶é—´èŒƒå›´+æ—¥å¿—çº§åˆ«]
    
    Extract1 --> Tool1[å·¥å…·: list_indices<br/>index_pattern=*log*]
    Tool1 --> Check1{æ£€æŸ¥: æ‰¾åˆ°ç´¢å¼•?}
    Check1 -->|å¦| Error1[ç»“æŸ: æœªæ‰¾åˆ°æ—¥å¿—ç´¢å¼•]
    Check1 -->|æ˜¯| Code1[ä»£ç : é€‰æ‹©æœ€æ–°çš„æ—¥å¿—ç´¢å¼•]
    
    Code1 --> Tool2[å·¥å…·: get_mappings<br/>index=é€‰å®šç´¢å¼•]
    Tool2 --> LLM2[LLM: æ¨æ–­å­—æ®µå<br/>timestamp + level]
    
    LLM2 --> Tool3[å·¥å…·: esql<br/>æ—¶åºèšåˆæŸ¥è¯¢]
    Tool3 --> Code2[ä»£ç : æ£€æµ‹å¼‚å¸¸å³°å€¼]
    
    Code2 --> Branch1{æ¡ä»¶: æœ‰å¼‚å¸¸å³°å€¼?}
    Branch1 -->|æ˜¯| Tool4[å·¥å…·: esql<br/>å¼‚å¸¸æ—¶æ®µæ·±æŒ–]
    Branch1 -->|å¦| Aggregate1[å˜é‡èšåˆå™¨]
    
    Tool4 --> Aggregate1
    Aggregate1 --> LLM3[LLM: ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š]
    LLM3 --> End[ç»“æŸ: è¾“å‡ºåˆ†æç»“æœ]
    
    Error1 --> End
```

### 4.3 å…³é”®èŠ‚ç‚¹é…ç½®

#### èŠ‚ç‚¹ 1: LLM - æ„å›¾æå–
```yaml
èŠ‚ç‚¹åç§°: intent_extraction
èŠ‚ç‚¹ç±»å‹: LLM
æ¨¡å‹: gpt-4o-mini (è½»é‡çº§å³å¯)
System Prompt:
  ä»ç”¨æˆ·è¾“å…¥ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œè¿”å› JSON:
  {
    "time_range": "7d",  // æ—¶é—´èŒƒå›´: 1h, 1d, 7d, 30d
    "log_level": "ERROR", // æ—¥å¿—çº§åˆ«: ERROR, WARN, INFO, ALL
    "keywords": ["è¶‹åŠ¿", "åˆ†æ"] // å…³é”®è¯
  }

User Message: {{workflow.user_query}}
è¾“å‡ºå˜é‡: intent.json
```

#### èŠ‚ç‚¹ 2: ä»£ç æ‰§è¡Œ - æ„é€ ç´¢å¼•æ¨¡å¼
```yaml
èŠ‚ç‚¹åç§°: build_index_pattern
èŠ‚ç‚¹ç±»å‹: ä»£ç æ‰§è¡Œ
ä»£ç :
```python
import json
from datetime import datetime, timedelta

intent = json.loads(intent_json)

# æ ¹æ®æ—¶é—´èŒƒå›´æ„é€ ç´¢å¼•æ¨¡å¼
time_range = intent.get("time_range", "7d")
if time_range.endswith("d"):
    days = int(time_range[:-1])
    # ç”Ÿæˆæœ€è¿‘ N å¤©çš„ç´¢å¼•åç¼€
    patterns = []
    for i in range(days):
        date = datetime.now() - timedelta(days=i)
        patterns.append(date.strftime("*log*%Y.%m*"))
    return {"index_pattern": ",".join(set(patterns))}
else:
    return {"index_pattern": "*log*"}
```
è¾“å‡ºå˜é‡: pattern.index_pattern
```

#### èŠ‚ç‚¹ 3: å·¥å…· - ES|QL æ—¶åºèšåˆ
```yaml
èŠ‚ç‚¹åç§°: esql_trend_query
èŠ‚ç‚¹ç±»å‹: å·¥å…·
å·¥å…·: elasticsearch-mcp.esql
å‚æ•°:
  query: |
    FROM {{pattern.index_pattern}}
    | WHERE @timestamp >= NOW() - {{intent.time_range}}
      {{#if intent.log_level != "ALL"}}
      AND log.level == "{{intent.log_level}}"
      {{/if}}
    | STATS error_count = COUNT(*) BY DATE_TRUNC(@timestamp, 1 DAY) AS day
    | SORT day ASC
è¾“å‡ºå˜é‡: trend.result
```

#### èŠ‚ç‚¹ 4: ä»£ç æ‰§è¡Œ - å¼‚å¸¸æ£€æµ‹
```yaml
èŠ‚ç‚¹åç§°: anomaly_detection
èŠ‚ç‚¹ç±»å‹: ä»£ç æ‰§è¡Œ
ä»£ç :
```python
import json
import numpy as np

data = json.loads(trend_result)
if not data:
    return {"has_anomaly": False}

# æå–è®¡æ•°
counts = [row["error_count"] for row in data]
mean = np.mean(counts)
std = np.std(counts)

# 3-sigma å¼‚å¸¸æ£€æµ‹
anomalies = []
for i, row in enumerate(data):
    if row["error_count"] > mean + 3 * std:
        anomalies.append({
            "date": row["day"],
            "count": row["error_count"],
            "threshold": mean + 3 * std
        })

return {
    "has_anomaly": len(anomalies) > 0,
    "anomalies": anomalies,
    "baseline": mean
}
```
è¾“å‡ºå˜é‡: 
  - anomaly.has_anomaly
  - anomaly.anomalies (æ•°ç»„)
```

---

## äº”ã€èƒ½åŠ› 3: ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯å¼æŸ¥è¯¢

### 5.1 å®ç°ç­–ç•¥

**Dify ç‰¹æ€§**: Dify çš„ **Chatflow** æ¨¡å¼è‡ªå¸¦ä¼šè¯è®°å¿†ï¼Œä½†éœ€è¦é€šè¿‡ **å˜é‡èšåˆ** æ˜¾å¼ç®¡ç†ä¸Šä¸‹æ–‡ã€‚

### 5.2 Workflow è®¾è®¡

```mermaid
graph TD
    Start[å¼€å§‹] --> Load[åŠ è½½ä¼šè¯ä¸Šä¸‹æ–‡<br/>conversation.history]
    Load --> LLM1[LLM: ç†è§£å½“å‰è¯·æ±‚<br/>+ ç»§æ‰¿ä¸Šä¸‹æ–‡]
    
    LLM1 --> Code1[ä»£ç : åˆå¹¶æŸ¥è¯¢æ¡ä»¶<br/>ä¸Šä¸€è½® + æœ¬è½®]
    Code1 --> Tool1[å·¥å…·: esql<br/>å®Œæ•´æŸ¥è¯¢]
    
    Tool1 --> Save[ä¿å­˜ä¸Šä¸‹æ–‡<br/>index+conditions]
    Save --> LLM2[LLM: ç”Ÿæˆå›å¤<br/>+ å¼•å¯¼ä¸‹ä¸€æ­¥]
    LLM2 --> End[ç»“æŸ]
```

### 5.3 å…³é”®èŠ‚ç‚¹é…ç½®

#### èŠ‚ç‚¹ 1: ä»£ç æ‰§è¡Œ - ä¸Šä¸‹æ–‡åˆå¹¶
```yaml
èŠ‚ç‚¹åç§°: merge_context
èŠ‚ç‚¹ç±»å‹: ä»£ç æ‰§è¡Œ
ä»£ç :
```python
import json

# åŠ è½½ä¸Šä¸€è½®çš„ä¸Šä¸‹æ–‡
prev_context = json.loads(conversation_context or "{}")

# å½“å‰è½®çš„æ–°æ¡ä»¶
current_query = llm_parsed_query  # æ¥è‡ª LLM

# åˆå¹¶è§„åˆ™:
# 1. index åç§°: å¦‚æœæœ¬è½®æ²¡æï¼Œç»§æ‰¿ä¸Šä¸€è½®
# 2. filters: è¿½åŠ è€Œä¸æ˜¯è¦†ç›–
merged = {
    "index": current_query.get("index") or prev_context.get("index"),
    "filters": prev_context.get("filters", []) + current_query.get("filters", []),
    "group_by": current_query.get("group_by")  # æœ¬è½®è¦†ç›–
}

return {"merged_context": json.dumps(merged)}
```
è¾“å‡ºå˜é‡: merged_context
```

#### èŠ‚ç‚¹ 2: å˜é‡èšåˆå™¨ - ä¿å­˜ä¸Šä¸‹æ–‡
```yaml
èŠ‚ç‚¹åç§°: save_conversation_context
èŠ‚ç‚¹ç±»å‹: å˜é‡èšåˆå™¨
èšåˆå†…å®¹:
  - {{merged_context}}
  - {{tool_result}}
ä¿å­˜åˆ°: conversation.context (ä¼šè¯å˜é‡)
```

---

## å…­ã€èƒ½åŠ› 4: å¤æ‚å…³è”åˆ†æ (Cross-Index)

### 6.1 å…¸å‹åœºæ™¯
**ç”¨æˆ·è¾“å…¥**: "åˆ†æè®¢å•å¤±è´¥ä¸æ”¯ä»˜ç½‘å…³å¼‚å¸¸çš„å…³è”æ€§"

### 6.2 Workflow è®¾è®¡

```mermaid
graph TD
    Start[å¼€å§‹] --> LLM1[LLM: è¯†åˆ«åŒç´¢å¼•éœ€æ±‚<br/>è®¢å• + æ”¯ä»˜]
    LLM1 --> Tool1[å·¥å…·: list_indices<br/>éªŒè¯ä¸¤ä¸ªç´¢å¼•å­˜åœ¨]
    
    Tool1 --> Branch1{æ£€æŸ¥: ç´¢å¼•éƒ½å­˜åœ¨?}
    Branch1 -->|å¦| Error1[é”™è¯¯: ç¼ºå¤±ç´¢å¼•]
    Branch1 -->|æ˜¯| Parallel[å¹¶è¡Œæ‰§è¡Œ]
    
    Parallel --> Tool2[å·¥å…·: esql<br/>è®¢å•å¤±è´¥æ—¶é—´åˆ†å¸ƒ]
    Parallel --> Tool3[å·¥å…·: esql<br/>æ”¯ä»˜å¼‚å¸¸æ—¶é—´åˆ†å¸ƒ]
    
    Tool2 --> Sync[åŒæ­¥ç‚¹: ç­‰å¾…ä¸¤ä¸ªæŸ¥è¯¢å®Œæˆ]
    Tool3 --> Sync
    
    Sync --> Code1[ä»£ç : è®¡ç®—æ—¶é—´é‡å ç‡<br/>ç›¸å…³ç³»æ•°]
    Code1 --> LLM2[LLM: ç”Ÿæˆå…³è”æŠ¥å‘Š]
    LLM2 --> End[ç»“æŸ]
    
    Error1 --> End
```

### 6.3 å…³é”®èŠ‚ç‚¹é…ç½®

#### èŠ‚ç‚¹ 1: å¹¶è¡Œæ‰§è¡Œ (Parallel)
```yaml
èŠ‚ç‚¹åç§°: parallel_queries
èŠ‚ç‚¹ç±»å‹: å¹¶è¡Œæ‰§è¡Œ (Parallel)
åˆ†æ”¯ 1: query_orders
  â””â”€ å·¥å…·: esql
     query: |
       FROM orders-*
       | WHERE status == "failed" AND @timestamp >= NOW() - 7d
       | STATS count = COUNT(*) BY DATE_TRUNC(@timestamp, 1h) AS hour
       | SORT hour ASC

åˆ†æ”¯ 2: query_payments
  â””â”€ å·¥å…·: esql
     query: |
       FROM payments-*
       | WHERE gateway_status == "error" AND @timestamp >= NOW() - 7d
       | STATS count = COUNT(*) BY DATE_TRUNC(@timestamp, 1h) AS hour
       | SORT hour ASC

åŒæ­¥æ¨¡å¼: ç­‰å¾…æ‰€æœ‰åˆ†æ”¯å®Œæˆ
è¾“å‡ºå˜é‡: 
  - orders_result
  - payments_result
```

#### èŠ‚ç‚¹ 2: ä»£ç æ‰§è¡Œ - å…³è”åˆ†æ
```yaml
èŠ‚ç‚¹åç§°: correlation_analysis
èŠ‚ç‚¹ç±»å‹: ä»£ç æ‰§è¡Œ
ä»£ç :
```python
import json
from datetime import datetime, timedelta

orders = json.loads(orders_result)
payments = json.loads(payments_result)

# æŒ‰å°æ—¶å¯¹é½
order_dict = {row["hour"]: row["count"] for row in orders}
payment_dict = {row["hour"]: row["count"] for row in payments}

# è®¡ç®—é‡å æ—¶æ®µ
overlap_count = 0
total_orders = len(orders)

for hour in order_dict:
    # å¦‚æœè®¢å•å¤±è´¥çš„å‰å 1 å°æ—¶å†…æœ‰æ”¯ä»˜å¼‚å¸¸
    for delta in [-1, 0, 1]:
        check_hour = (datetime.fromisoformat(hour) + timedelta(hours=delta)).isoformat()
        if check_hour in payment_dict and payment_dict[check_hour] > 0:
            overlap_count += 1
            break

overlap_rate = (overlap_count / total_orders * 100) if total_orders > 0 else 0

return {
    "overlap_rate": round(overlap_rate, 2),
    "total_order_failures": sum(order_dict.values()),
    "total_payment_errors": sum(payment_dict.values())
}
```
è¾“å‡ºå˜é‡: 
  - correlation.overlap_rate
  - correlation.total_order_failures
```

---

## ä¸ƒã€é€šç”¨ç»„ä»¶ä¸æœ€ä½³å®è·µ

### 7.1 å¯å¤ç”¨å­å·¥ä½œæµ

#### å­å·¥ä½œæµ 1: ES è¿æ¥å¥åº·æ£€æŸ¥
```yaml
åç§°: subflow_health_check
è¾“å…¥: æ— 
è¾“å‡º: 
  - is_healthy (Boolean)
  - error_message (String)

èŠ‚ç‚¹:
  1. å·¥å…·: get_cluster_health (è¶…æ—¶ 5 ç§’)
  2. æ¡ä»¶åˆ†æ”¯: 
     - å¦‚æœæˆåŠŸ â†’ is_healthy = true
     - å¦‚æœå¤±è´¥ â†’ is_healthy = false, è®°å½•é”™è¯¯
```

**ä½¿ç”¨åœºæ™¯**: åœ¨æ¯ä¸ªä¸»å·¥ä½œæµçš„å¼€å¤´è°ƒç”¨ï¼Œç¡®ä¿ ES å¯è¾¾åå†æ‰§è¡Œå¤æ‚é€»è¾‘ã€‚

#### å­å·¥ä½œæµ 2: ç´¢å¼•è‡ªåŠ¨è¯†åˆ«
```yaml
åç§°: subflow_index_discovery
è¾“å…¥: 
  - keywords (Array): å…³é”®è¯åˆ—è¡¨, å¦‚ ["log", "error"]
è¾“å‡º: 
  - matched_indices (Array)

èŠ‚ç‚¹:
  1. å·¥å…·: list_indices(index_pattern="*")
  2. ä»£ç : è¿‡æ»¤åŒ…å« keywords çš„ç´¢å¼•
  3. ä»£ç : æŒ‰æœ€åä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å› TOP 5
```

### 7.2 é”™è¯¯å¤„ç†æ¨¡æ¿

åœ¨æ¯ä¸ªå·¥å…·èŠ‚ç‚¹åæ·»åŠ :

```yaml
èŠ‚ç‚¹: tool_call
  â†“
æ¡ä»¶åˆ†æ”¯: {{tool_call.error}} == null
  â”œâ”€ TRUE: ç»§ç»­æ‰§è¡Œ
  â””â”€ FALSE: è·³è½¬åˆ° error_handler

error_handler (LLM èŠ‚ç‚¹):
  System Prompt: |
    å·¥å…·è°ƒç”¨å¤±è´¥: {{tool_call.error}}
    
    è¯·ç”Ÿæˆç”¨æˆ·å‹å¥½çš„é”™è¯¯æç¤º:
    1. è¯´æ˜ä»€ä¹ˆå¤±è´¥äº†
    2. å¯èƒ½çš„åŸå›  (ES æœåŠ¡åœæ­¢/ç½‘ç»œé—®é¢˜/è®¤è¯å¤±è´¥)
    3. æ’æŸ¥æ­¥éª¤
  
  è¾“å‡º: ç›´æ¥è¿”å›ç»™ç”¨æˆ·ï¼Œæå‰ç»“æŸå·¥ä½œæµ
```

### 7.3 å˜é‡å‘½åæœ€ä½³å®è·µ

| å‘½åç©ºé—´ | ç¤ºä¾‹ | ç”¨é€” |
|---------|------|------|
| `workflow.*` | `workflow.user_query` | å·¥ä½œæµå…¨å±€å˜é‡ |
| `<èŠ‚ç‚¹å>.*` | `health_check.status` | èŠ‚ç‚¹è¾“å‡ºå˜é‡ |
| `temp.*` | `temp.parsed_json` | ä¸´æ—¶ä¸­é—´å˜é‡ |
| `context.*` | `context.current_index` | ä¼šè¯ä¸Šä¸‹æ–‡ |

### 7.4 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1.  **å·¥å…·è°ƒç”¨å¹¶è¡ŒåŒ–**: å½“å¤šä¸ªå·¥å…·è°ƒç”¨æ— ä¾èµ–å…³ç³»æ—¶ï¼Œä½¿ç”¨ **Parallel èŠ‚ç‚¹**ã€‚
    ```
    å¹¶è¡ŒæŸ¥è¯¢:
      â”œâ”€ åˆ†æ”¯ 1: get_nodes_info
      â””â”€ åˆ†æ”¯ 2: list_indices_detailed
    ```

2.  **ç»“æœç¼“å­˜**: å¯¹äºçŸ­æœŸå†…ä¸å˜çš„æ•°æ®(å¦‚ç´¢å¼•åˆ—è¡¨)ï¼Œä½¿ç”¨ **ä¼šè¯å˜é‡ç¼“å­˜**ã€‚
    ```python
    if context.get("indices_cache_time") > now() - 300:  # 5 åˆ†é’Ÿç¼“å­˜
        return context["indices_cache"]
    ```

3.  **LLM è°ƒç”¨ä¼˜åŒ–**: 
    *   æ„å›¾è¯†åˆ«ç”¨ `gpt-4o-mini`
    *   å¤æ‚æ¨ç†ç”¨ `gpt-4o` æˆ– `claude-3.5-sonnet`
    *   æ ¼å¼åŒ–è¾“å‡ºç”¨ **ä»£ç èŠ‚ç‚¹** è€Œä¸æ˜¯ LLM

---

## å…«ã€éƒ¨ç½²ä¸ç›‘æ§

### 8.1 å·¥ä½œæµå‘å¸ƒæ¸…å•

- [ ] æ‰€æœ‰å·¥å…·èŠ‚ç‚¹è®¾ç½®è¶…æ—¶æ—¶é—´ (10-30 ç§’)
- [ ] é”™è¯¯å¤„ç†åˆ†æ”¯è¦†ç›–ç‡ > 80%
- [ ] å…³é”®èŠ‚ç‚¹æ·»åŠ æ—¥å¿—è¾“å‡º
- [ ] æµ‹è¯• 3 ç§å…¸å‹åœºæ™¯ (æ­£å¸¸/å¼‚å¸¸/è¾¹ç•Œ)
- [ ] é…ç½®å·¥ä½œæµè§¦å‘å™¨ (Webhook/å®šæ—¶ä»»åŠ¡/æ‰‹åŠ¨)

### 8.2 ç›‘æ§æŒ‡æ ‡

åœ¨ Dify æ§åˆ¶å°å…³æ³¨:

| æŒ‡æ ‡ | å¥åº·é˜ˆå€¼ | å‘Šè­¦é˜ˆå€¼ |
|------|---------|---------|
| å·¥ä½œæµæ‰§è¡Œæ—¶é—´ | < 30 ç§’ | > 60 ç§’ |
| å·¥å…·è°ƒç”¨æˆåŠŸç‡ | > 95% | < 90% |
| LLM Token æ¶ˆè€— | < 5000 tokens/æ¬¡ | > 10000 tokens/æ¬¡ |
| é”™è¯¯ç‡ | < 5% | > 10% |

### 8.3 æ—¥å¿—è®°å½•å»ºè®®

åœ¨å…³é”®èŠ‚ç‚¹æ·»åŠ  **ä»£ç æ‰§è¡ŒèŠ‚ç‚¹** è®°å½•æ—¥å¿—:

```python
import json
from datetime import datetime

log_entry = {
    "timestamp": datetime.now().isoformat(),
    "node": "health_check",
    "action": "tool_call",
    "result": "success" if health_check.error is None else "failed",
    "data": json.dumps(health_check.result)
}

# è¾“å‡ºåˆ°å·¥ä½œæµæ—¥å¿—
print(json.dumps(log_entry))
return {"logged": True}
```

### 8.4 ç‰ˆæœ¬ç®¡ç†

å»ºè®®ä½¿ç”¨ Git ç®¡ç†å·¥ä½œæµé…ç½®:

```bash
mcp-workflows/
â”œâ”€â”€ es-health-diagnosis/
â”‚   â”œâ”€â”€ workflow.json          # Dify å¯¼å‡ºçš„é…ç½®
â”‚   â”œâ”€â”€ README.md              # ä½¿ç”¨æ–‡æ¡£
â”‚   â””â”€â”€ test-cases.json        # æµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ es-data-exploration/
â””â”€â”€ es-correlation-analysis/
```

---

## ä¹ã€å¿«é€Ÿå¼€å§‹: éƒ¨ç½²ç¬¬ä¸€ä¸ªå·¥ä½œæµ

### æ­¥éª¤ 1: å¯¼å…¥å·¥ä½œæµæ¨¡æ¿

1.  åœ¨ Dify æ§åˆ¶å°åˆ›å»ºæ–°å·¥ä½œæµ
2.  é€‰æ‹© "ä» JSON å¯¼å…¥"
3.  ç²˜è´´ä»¥ä¸‹æœ€å°åŒ–æ¨¡æ¿:

```json
{
  "nodes": [
    {"id": "start", "type": "start"},
    {"id": "health", "type": "tool", "tool": "elasticsearch-mcp.get_cluster_health"},
    {"id": "llm", "type": "llm", "prompt": "åˆ†æé›†ç¾¤çŠ¶æ€: {{health.result}}"},
    {"id": "end", "type": "end"}
  ],
  "edges": [
    {"source": "start", "target": "health"},
    {"source": "health", "target": "llm"},
    {"source": "llm", "target": "end"}
  ]
}
```

### æ­¥éª¤ 2: é…ç½® MCP å·¥å…·

åœ¨å·¥ä½œæµè®¾ç½®ä¸­:
1.  æ·»åŠ  `elasticsearch-mcp` åˆ°å¯ç”¨å·¥å…·åˆ—è¡¨
2.  è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 10 ç§’
3.  å¯ç”¨ "å¤±è´¥æ—¶ç»§ç»­"

### æ­¥éª¤ 3: æµ‹è¯•

è¾“å…¥æµ‹è¯•ç”¨ä¾‹: "æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€"

é¢„æœŸè¾“å‡º:
```
é›†ç¾¤çŠ¶æ€: Green
èŠ‚ç‚¹æ•°: 3
åˆ†ç‰‡åˆ†é…ç‡: 100%
å»ºè®®: é›†ç¾¤è¿è¡Œæ­£å¸¸
```

### æ­¥éª¤ 4: è¿­ä»£ä¼˜åŒ–

1.  æ·»åŠ æ¡ä»¶åˆ†æ”¯ (Red/Yellow/Green)
2.  å¢åŠ èŠ‚ç‚¹ä¿¡æ¯æŸ¥è¯¢
3.  ä¸°å¯Œ LLM çš„åˆ†ææç¤ºè¯

---

## åã€æ€»ç»“ä¸ä¸‹ä¸€æ­¥

### å·²å®ç°çš„èƒ½åŠ›
âœ… æ•…éšœè¯Šæ–­å·¥ä½œæµ (8-12 èŠ‚ç‚¹)
âœ… æ•°æ®æ¢ç´¢å·¥ä½œæµ (10-15 èŠ‚ç‚¹)
âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢ (Chatflow æ¨¡å¼)
âœ… å…³è”åˆ†æå·¥ä½œæµ (å¹¶è¡Œæ‰§è¡Œ)

### å¾…ä¼˜åŒ–æ–¹å‘
ğŸ”„ RAG çŸ¥è¯†åº“é›†æˆ (åœºæ™¯ 2)
ğŸ”„ è‡ªåŠ¨ä¿®å¤èƒ½åŠ› (ä¸ Ansible MCP è”åŠ¨)
ğŸ”„ é¢„æµ‹æ€§ç»´æŠ¤ (åŸºäºå†å²æ•°æ®è¶‹åŠ¿)

### æŠ€æœ¯å€ºç®¡ç†
*   å·¥ä½œæµæ‰§è¡Œæ—¥å¿—æŒä¹…åŒ–
*   å¼‚å¸¸åœºæ™¯è¦†ç›–ç‡æå‡åˆ° 95%
*   LLM æç¤ºè¯ç‰ˆæœ¬æ§åˆ¶
*   å·¥ä½œæµ A/B æµ‹è¯•æ¡†æ¶

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-01-22
**ç»´æŠ¤è€…**: AIOps Team
